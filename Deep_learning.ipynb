{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahulk98/Deep_learning_algorithms/blob/master/Deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwcTPCVlLasA",
        "colab_type": "code",
        "outputId": "06c122a7-e76e-4571-d694-6ef4f2d2c19d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "#Import the libraries required\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "import time\n",
        "\n",
        "\n",
        "#Splitting the data to test and train\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "#Convert the images to vectors by flattening them\n",
        "image_vector_size = 28*28\n",
        "x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
        "x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
        "\n",
        "\n",
        "\n",
        "#Convert the output to one-hot encoded vectors\n",
        "num_classes = 10                                          \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "#Creating a sequential model \n",
        "model = Sequential()\n",
        "#Adding the first hidden layer with 32 nodes. The input layer has 784 nodes\n",
        "model.add(Dense(units=397, activation='sigmoid', kernel_initializer='uniform', input_shape=(image_vector_size,)))\n",
        "#Adding the output layer\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "model.summary()\n",
        "\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "#Training the model\n",
        "\n",
        "model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=15, verbose=False, validation_split=.1)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
        "print(accuracy)\n",
        "\n",
        "time_taken = end - start                                                                #time taken\n",
        "print('Time taken: ',time_taken)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 397)               311645    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                3980      \n",
            "=================================================================\n",
            "Total params: 315,625\n",
            "Trainable params: 315,625\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "0.9548\n",
            "Time taken:  43.91749858856201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHe0EzZQdkop",
        "colab_type": "code",
        "outputId": "b56a7578-cdde-49e5-a1e7-370f3d4f75a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn import linear_model, datasets, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import clone\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Turn down for faster convergence\n",
        "train_samples = 60000\n",
        "\n",
        "# Load data from https://www.openml.org/d/554\n",
        "x, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
        "\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y, train_size=train_samples, test_size=10000)\n",
        "\n",
        "##############################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "\n",
        "x_train = x_train/255 # 0-1 scaling\n",
        "x_test =  x_test/255\n",
        "y_train = y_train\n",
        "y_test = y_test\n",
        "\n",
        "\n",
        "logistic = linear_model.LogisticRegression(solver='newton-cg', tol=1,\n",
        "                                           multi_class='multinomial')\n",
        "rbm = BernoulliRBM(random_state=0, verbose=True)\n",
        "\n",
        "rbm_features_classifier = Pipeline(\n",
        "    steps=[('rbm', rbm), ('logistic', logistic)])\n",
        "\n",
        "\n",
        "\n",
        "# #############################################################################\n",
        "# Training\n",
        "\n",
        "rbm.learning_rate = 0.005\n",
        "rbm.n_iter = 7\n",
        "# More components tend to give better prediction performance, but larger\n",
        "# fitting time\n",
        "rbm.n_components = 100\n",
        "logistic.C = 10.0\n",
        "\n",
        "# Training RBM-Logistic Pipeline\n",
        "\n",
        "\n",
        "rbm_features_classifier.fit(x_train, y_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # #############################################################################\n",
        "# # Evaluation\n",
        "\n",
        "y_pred = rbm_features_classifier.predict(x_test) \n",
        "print(\"Logistic regression using RBM features:\\n%s\\n\" % (\n",
        "    metrics.accuracy_score(y_test, y_pred)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -129.49, time = 14.08s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -111.77, time = 15.75s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -103.77, time = 15.78s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -98.12, time = 15.86s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -94.43, time = 15.74s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -91.48, time = 15.80s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -89.75, time = 15.73s\n",
            "Logistic regression using RBM features:\n",
            "0.9317\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB_ESYcTZi0S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "62cb1094-9d2d-48f4-99d3-5cd59d827768"
      },
      "source": [
        "!pip install git+git://github.com/albertbup/deep-belief-network.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/albertbup/deep-belief-network.git\n",
            "  Cloning git://github.com/albertbup/deep-belief-network.git to /tmp/pip-req-build-u4yq4fhk\n",
            "  Running command git clone -q git://github.com/albertbup/deep-belief-network.git /tmp/pip-req-build-u4yq4fhk\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (0.21.1)\n",
            "Requirement already satisfied: tensorflow>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.13.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->deep-belief-network==1.0.3) (0.12.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.7.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.13.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.0.9)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.13.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.5.0->deep-belief-network==1.0.3) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow>=1.5.0->deep-belief-network==1.0.3) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (0.15.4)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow>=1.5.0->deep-belief-network==1.0.3) (3.0.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow>=1.5.0->deep-belief-network==1.0.3) (2.8.0)\n",
            "Building wheels for collected packages: deep-belief-network\n",
            "  Building wheel for deep-belief-network (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7gjbw5cm/wheels/29/6d/3b/6a50cf42a32bdfaa903b17832d60d8d3e5dc4b0fd02ae8acaf\n",
            "Successfully built deep-belief-network\n",
            "Installing collected packages: deep-belief-network\n",
            "Successfully installed deep-belief-network-1.0.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPJPVqJFPhP0",
        "colab_type": "code",
        "outputId": "85f350f9-4c39-49d0-d21d-f87040a21571",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2281
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "np.random.seed(1337)  # for reproducibility\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.classification import accuracy_score\n",
        "\n",
        "from dbn.tensorflow import SupervisedDBNClassification\n",
        "\n",
        "# Loading dataset\n",
        "digits = load_digits()\n",
        "X, Y = digits.data, digits.target\n",
        "\n",
        "# Data scaling\n",
        "X = (X / 16).astype(np.float32)\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Training\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.1,\n",
        "                                         n_epochs_rbm=10,\n",
        "                                         n_iter_backprop=100,\n",
        "                                         batch_size=32,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Test\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dbn/tensorflow/models.py:150: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 3.221450\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 2.169739\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.828885\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.628937\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 1.407109\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 1.287342\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 1.186931\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 1.098507\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 1.038850\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.969662\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2.863971\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 1.770083\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.227978\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.014690\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.984450\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.776238\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.704292\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.596336\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.646644\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.634897\n",
            "[END] Pre-training step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/dbn/tensorflow/models.py:338: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 0 finished \tANN training loss 1.346230\n",
            ">> Epoch 1 finished \tANN training loss 0.724887\n",
            ">> Epoch 2 finished \tANN training loss 0.533096\n",
            ">> Epoch 3 finished \tANN training loss 0.451627\n",
            ">> Epoch 4 finished \tANN training loss 0.354878\n",
            ">> Epoch 5 finished \tANN training loss 0.307330\n",
            ">> Epoch 6 finished \tANN training loss 0.298794\n",
            ">> Epoch 7 finished \tANN training loss 0.246707\n",
            ">> Epoch 8 finished \tANN training loss 0.224059\n",
            ">> Epoch 9 finished \tANN training loss 0.221503\n",
            ">> Epoch 10 finished \tANN training loss 0.195660\n",
            ">> Epoch 11 finished \tANN training loss 0.187591\n",
            ">> Epoch 12 finished \tANN training loss 0.197890\n",
            ">> Epoch 13 finished \tANN training loss 0.182189\n",
            ">> Epoch 14 finished \tANN training loss 0.148816\n",
            ">> Epoch 15 finished \tANN training loss 0.163741\n",
            ">> Epoch 16 finished \tANN training loss 0.146055\n",
            ">> Epoch 17 finished \tANN training loss 0.137736\n",
            ">> Epoch 18 finished \tANN training loss 0.139008\n",
            ">> Epoch 19 finished \tANN training loss 0.135918\n",
            ">> Epoch 20 finished \tANN training loss 0.122771\n",
            ">> Epoch 21 finished \tANN training loss 0.124440\n",
            ">> Epoch 22 finished \tANN training loss 0.113366\n",
            ">> Epoch 23 finished \tANN training loss 0.114498\n",
            ">> Epoch 24 finished \tANN training loss 0.103856\n",
            ">> Epoch 25 finished \tANN training loss 0.106004\n",
            ">> Epoch 26 finished \tANN training loss 0.098406\n",
            ">> Epoch 27 finished \tANN training loss 0.094060\n",
            ">> Epoch 28 finished \tANN training loss 0.097687\n",
            ">> Epoch 29 finished \tANN training loss 0.092050\n",
            ">> Epoch 30 finished \tANN training loss 0.091898\n",
            ">> Epoch 31 finished \tANN training loss 0.095301\n",
            ">> Epoch 32 finished \tANN training loss 0.085697\n",
            ">> Epoch 33 finished \tANN training loss 0.080282\n",
            ">> Epoch 34 finished \tANN training loss 0.079140\n",
            ">> Epoch 35 finished \tANN training loss 0.078644\n",
            ">> Epoch 36 finished \tANN training loss 0.076583\n",
            ">> Epoch 37 finished \tANN training loss 0.080906\n",
            ">> Epoch 38 finished \tANN training loss 0.071388\n",
            ">> Epoch 39 finished \tANN training loss 0.083600\n",
            ">> Epoch 40 finished \tANN training loss 0.067704\n",
            ">> Epoch 41 finished \tANN training loss 0.069863\n",
            ">> Epoch 42 finished \tANN training loss 0.074818\n",
            ">> Epoch 43 finished \tANN training loss 0.073897\n",
            ">> Epoch 44 finished \tANN training loss 0.059130\n",
            ">> Epoch 45 finished \tANN training loss 0.065800\n",
            ">> Epoch 46 finished \tANN training loss 0.062911\n",
            ">> Epoch 47 finished \tANN training loss 0.061888\n",
            ">> Epoch 48 finished \tANN training loss 0.058899\n",
            ">> Epoch 49 finished \tANN training loss 0.059081\n",
            ">> Epoch 50 finished \tANN training loss 0.073639\n",
            ">> Epoch 51 finished \tANN training loss 0.054452\n",
            ">> Epoch 52 finished \tANN training loss 0.059245\n",
            ">> Epoch 53 finished \tANN training loss 0.061211\n",
            ">> Epoch 54 finished \tANN training loss 0.057634\n",
            ">> Epoch 55 finished \tANN training loss 0.057880\n",
            ">> Epoch 56 finished \tANN training loss 0.053329\n",
            ">> Epoch 57 finished \tANN training loss 0.055200\n",
            ">> Epoch 58 finished \tANN training loss 0.054090\n",
            ">> Epoch 59 finished \tANN training loss 0.060462\n",
            ">> Epoch 60 finished \tANN training loss 0.049711\n",
            ">> Epoch 61 finished \tANN training loss 0.056995\n",
            ">> Epoch 62 finished \tANN training loss 0.064019\n",
            ">> Epoch 63 finished \tANN training loss 0.060056\n",
            ">> Epoch 64 finished \tANN training loss 0.045070\n",
            ">> Epoch 65 finished \tANN training loss 0.053201\n",
            ">> Epoch 66 finished \tANN training loss 0.040954\n",
            ">> Epoch 67 finished \tANN training loss 0.047181\n",
            ">> Epoch 68 finished \tANN training loss 0.046248\n",
            ">> Epoch 69 finished \tANN training loss 0.045828\n",
            ">> Epoch 70 finished \tANN training loss 0.048861\n",
            ">> Epoch 71 finished \tANN training loss 0.043860\n",
            ">> Epoch 72 finished \tANN training loss 0.059608\n",
            ">> Epoch 73 finished \tANN training loss 0.043755\n",
            ">> Epoch 74 finished \tANN training loss 0.042520\n",
            ">> Epoch 75 finished \tANN training loss 0.039846\n",
            ">> Epoch 76 finished \tANN training loss 0.039146\n",
            ">> Epoch 77 finished \tANN training loss 0.048068\n",
            ">> Epoch 78 finished \tANN training loss 0.058851\n",
            ">> Epoch 79 finished \tANN training loss 0.041173\n",
            ">> Epoch 80 finished \tANN training loss 0.036762\n",
            ">> Epoch 81 finished \tANN training loss 0.040686\n",
            ">> Epoch 82 finished \tANN training loss 0.036051\n",
            ">> Epoch 83 finished \tANN training loss 0.042036\n",
            ">> Epoch 84 finished \tANN training loss 0.039022\n",
            ">> Epoch 85 finished \tANN training loss 0.031211\n",
            ">> Epoch 86 finished \tANN training loss 0.033952\n",
            ">> Epoch 87 finished \tANN training loss 0.040892\n",
            ">> Epoch 88 finished \tANN training loss 0.034535\n",
            ">> Epoch 89 finished \tANN training loss 0.037418\n",
            ">> Epoch 90 finished \tANN training loss 0.037979\n",
            ">> Epoch 91 finished \tANN training loss 0.037377\n",
            ">> Epoch 92 finished \tANN training loss 0.033090\n",
            ">> Epoch 93 finished \tANN training loss 0.034323\n",
            ">> Epoch 94 finished \tANN training loss 0.039070\n",
            ">> Epoch 95 finished \tANN training loss 0.032923\n",
            ">> Epoch 96 finished \tANN training loss 0.031653\n",
            ">> Epoch 97 finished \tANN training loss 0.030196\n",
            ">> Epoch 98 finished \tANN training loss 0.035345\n",
            ">> Epoch 99 finished \tANN training loss 0.043220\n",
            "[END] Fine tuning step\n",
            "Done.\n",
            "Accuracy: 0.983333\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}